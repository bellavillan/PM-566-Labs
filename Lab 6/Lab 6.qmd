---
title: "Lab 6"
author: Isabella Villanueva 
output: html
embed-resources: true
theme: flatly 
---

# Lab 6: Text Mining

#### Loading in the `mt_samples` dataset.

```{r}
library(dplyr)
library(data.table)
library(ggplot2)
library(tidytext)
```

```{r}
library(readr)
library(dplyr)
mt_samples <- read_csv("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/00_mtsamples/mtsamples.csv")
mt_samples <- mt_samples %>%
  select(description, medical_specialty, transcription)

head(mt_samples)
```

## Question 1. What specialities do we have?

```{r}
colnames(mt_samples)
```

```{r}
library(dplyr)
mt_samples <- mt_samples %>% 
  rename(desc = description, #Renaming variables for convenience
         med_spec = medical_specialty,
         tran = transcription)
```

```{r}
mt_samples %>%
  count(desc, sort = TRUE)
mt_samples %>%
  count(med_spec, sort = TRUE)
mt_samples %>%
  count(tran, sort = TRUE)
```

### How many different categories do we have? Are these categories related? overlapping? evenly distributed?

From the `count` function, we can analyze the `mt_samples` dataset's 3 different variables.

Within the `description` variable (renamed as `desc`) and the `transcription` variable (renamed as `tran`), 2,348 unique categories were counted, each with their own written note on a medical phenomenon that could occur in a patient. These two variables overlap because the description informs the transcription and will not have duplicates.

Within the `medical_speciality` variable (renamed as `med_spec`), 40 unique categories were counted while this variable included an `n` column, which counts the number of overlapping observations with the same categorization (i.e. 1103 observations were categorized as "Surgery" and 516 observations were labeled "Consult- History and Phy.").

Within the `transcription` variable, many of the categories are related as many of the transcriptions do begin with the phrase "CHIEF COMPLAINT" and include "HISTORY OF PRESENT ILLNESS", yet are unique from each other to not overlap.

Not all of these variables have evenly distributed categories, as `medical_speciality` does have higher proportions of its variable within one categories than another.

## Question 2.

#### - Tokenize the words in the transcription column

```{r}
library(tidytext)
mt_samples |>
  unnest_tokens(token, tran)
```

#### - Count the number of times each token appears

```{r}
library(tidytext)
mt_samples |>
  unnest_tokens(token, tran) |>
  count(token, sort = TRUE)
```

#### - Visualize the top 20 most frequent words

```{r}
library(tidytext)
library(dplyr)
library(ggplot2)
library(forcats)
mt_samples |>
  unnest_tokens(token, tran) |>
  count(token, sort = TRUE) |>
  top_n(20,n)|>
  ggplot(aes(n, fct_reorder(token, n))) +
  geom_col()
```

### Explain what we see from this result. Does it makes sense? What insights (if any) do we get?

We can immediately see an explosion of observations in the `token` table compared to the original `transcription` column (from 2,348 categories to 2,403,596). This is understandable as tokenizing involves the conversion from the original text description of the `transcription` column to individual words as tokens. This can tell us that in the 2,348 categories, over 2 million words were included in the medical transcriptions.

From tokenizing the words of the `transcription` column, we were able to see the number of times 23,647 words appeared in this column. With the `top_n` function, we can visualize the 20 most common words in this column's data, and how many times each word appeared. The word "the" was used 149,888 times, followed by the word "and" at half its number of observed times!

## Question 3.

#### - Redo visualization but remove `stop_words` before

#### - Bonus points if you remove numbers as well

```{r}
library(tidytext)
library(dplyr)
library(ggplot2)
library(forcats)
library(stringr)
stop_words
mt_samples |>
  unnest_tokens(token, tran) |>
  anti_join(stop_words, by = c("token" = "word")) |> #Removal of stop_words
  filter(!str_detect(token, "^\\d+$")) |> #Removal of numbers
  count(token, sort = TRUE) |>
  top_n(20,n)|>
  ggplot(aes(n, fct_reorder(token, n))) +
  geom_col()
```

### What do we see know that we have removed stop words? Does it give us a better idea of what the text is about?

Now that we have removed the `stop_words`, the previously high ranking word "the" has been dethroned by the word "patient", which allows more insight into the data input from the transcription column in our dataset. Seeing this visualization with the noise eliminated from the filler words or stop words, we can notice more medical terminology in these top 20 most used words graphic.

## Question 4.

#### Repeat question 2, but this time tokenize into bi-grams.

```{r}
library(tidytext)
library(dplyr)
library(ggplot2)
library(forcats)
mt_samples |>
  unnest_ngrams(ngram,tran, n =2) |> #Instead of creating words into tokens, making two word phrases = bigrams
  count(ngram, sort = TRUE) |>
  top_n(20,n)|>
  ggplot(aes(n, fct_reorder(ngram, n))) +
  geom_col()
```

### How does the result change if you look at tri-grams?

```{r}
library(tidytext)
library(dplyr)
library(ggplot2)
library(forcats)
mt_samples |>
  unnest_ngrams(ngram,tran, n =3) |> #Instead of creating words into tokens, making two word phrases = trigrams
  count(ngram, sort = TRUE) |>
  top_n(20,n)|>
  ggplot(aes(n, fct_reorder(ngram, n))) +
  geom_col()
```

#### Bi-grams vs. Tri-grams

The comparison between the bi-grams and tri-grams shows a more specific output than the initial result from Question 2, where patient was the most observed word in this `transcription` column. Now, looking at the tri-grams compared to the bi-grams, the tri-grams or three letter phrases most used in this column are:

-   "the patient was",

-   "the patient is", and

-   "as well as".

The bi-grams graph had the same token at its most used, "patient", but with the specification of two words to three, more information can be collected on the mode of phrases compared to the bi-grams' graphic showing normally used phrases like:

-   "of the",

-   "in the", and

-   "to the".

The bi-grams result has been readjusted with the tri-grams output, where it bears similar results to before and after the removal of stop words.

### Question 5. Using the results you got from Question 4, pick a word and count the words that appears after and before it.

#### Chosen Word: "patient"

#### Words that appear after "patient"

```{r}
library(tidytext)
library(tidyr)
mt_samples |>
  unnest_ngrams(ngram, tran, n = 2) |>
  separate(ngram, into = c("word1", "word2"), sep = " ") |>
  select(word1, word2) |>
  filter(word1 == "patient") |>
  count(word2, sort = TRUE) #words that appear after "patient"
```

#### Words that appear before "patient"

```{r}
library(tidytext)
mt_samples |>
  unnest_ngrams(ngram, tran, n = 2) |>
  separate(ngram, into = c("word1", "word2"), sep = " ") |>
  select(word1, word2) |>
  filter(word2 == "patient") |>
  count(word1, sort = TRUE)
```

### Question 6.

#### Which words are most used in each of the specialties? How about the most 5 used words?
##### You can use `group_by`and `top_n` from dplyr to have the calculations be done within each specialty. Remember to remove stop words.

##### Below shows the top 5 most used words across each of the specialities.
```{r}
library(tidytext)
library(dplyr)
library(forcats)
library(stringr)
library(ggplot2)
med_spec_mostused <- mt_samples |>
  unnest_tokens(token, tran) |>
  anti_join(stop_words, by = c("token" = "word")) |>
  filter(!str_detect(token, "^\\d+$")) |>
  group_by(med_spec) |>
  count(token, sort = TRUE)|>
  top_n(5, n)|>
  slice_max(order_by = n, n = 5)

print(med_spec_mostused)
```
