---
title: "Lab 9: HPC"
author: Isabella Villanueva 
output: html
embed-resources: true
theme: flatly 
---

### Loading libraries 

```{r}
library(microbenchmark)
library(parallel)
library(dplyr)
library(tidyverse)
```

## Problem 1: Vectorization 

```{r}
# "Slow" Function
fun1 <- function(n = 100, k = 4, lambda = 4) {
  x <- NULL
  for (i in 1:n){
    x <- rbind(x, rpois(k, lambda))}    
  return(x)}

# "Fast" Function
fun1alt <- function(n = 100, k = 4, lambda = 4) {
x <- matrix(rpois(n * k, lambda), nrow = n, ncol = k)
return(x)}

#Showing that fun1alt generates a matrix with the same dimensions as fun1
dim(fun1())
dim(fun1alt())

#Checking identical outputs of both matrices
set.seed(123)
matrix1 <- fun1(n = 100, k = 4, lambda = 4)
matrix1alt <- fun1alt(n = 100, k = 4, lambda = 4)
identical(dim(matrix1), dim(matrix1alt))

#Showing the Speed -- Benchmarking
microbenchmark::microbenchmark(
  fun1(),
  fun1alt())
```
## Problem 2. Find the maximum value of each column of a matrix

```{r}
# Data Generating Process (10 x 10,000 matrix)
set.seed(1234)
x <- matrix(rnorm(1e4), nrow=10)

# Find each column's max value
fun2 <- function(x) {
  apply(x, 2, max)
}

fun2alt <- function(x) {
   x[cbind(max.col(t(x), ties.method = "first"), 1:ncol(x))] #Using function max.col
}
```

Show that both functions return the same output for a given input matrix, x. Then check the speed of the two functions. 

```{r}
# Generate a matrix as per your example
set.seed(1234)
x <- matrix(rnorm(1e4), nrow = 10)

# Check if outputs are identical
identical(fun2(x), fun2alt(x))  # Should return TRUE
```

```{r}
library(microbenchmark)
microbenchmark( #Comparing the speed 
  original = fun2(x),
  optimized = fun2alt(x),
  times = 100
)
```

## Problem 3: Parallelization

### 1. This function implements a serial version of the bootstrap. Edit this function to parallelize the lapply loop, using whichever method you prefer. Rather than specifying the number of cores to use, use the number given by the ncpus argument, so that we can test it with different numbers of cores later.
```{r}
my_boot <- function(dat, stat, R, ncpus = 1L) {
  
  # Getting the random indices
  n <- nrow(dat)
  idx <- matrix(sample.int(n, n*R, TRUE), nrow=n, ncol=R)
  
  # Parallelized function
  ans <- mclapply(seq_len(R), function(i) {
   result <- stat(dat[idx[,i], , drop=FALSE])
  }, mc.cores = ncpus)
  
  # Converting the list into a matrix
  ans <- do.call(rbind, ans)
  return(ans)
}
```

### 2. Once you have a version of the my_boot() function that runs on multiple cores, check that it provides accurate results by comparing it to a parametric model:

```{r}
# Bootstrap of an OLS
my_stat <- function(d) coef(lm(y ~ x, data=d))

# DATA SIM
set.seed(1)
n <- 500; R <- 1e4

x <- cbind(rnorm(n)); y <- x*5 + rnorm(n)

# Checking if we get something similar as lm
ans0 <- confint(lm(y~x))
ans1 <- my_boot(dat = data.frame(x, y), my_stat, R = R, ncpus = 2L)

t(apply(ans1, 2, quantile, c(.025,.975)))
ans0
```

### 3. Check whether your version actually goes faster when it’s run on multiple cores (since this might take a little while to run, we’ll use system.time and just run each version once, rather than microbenchmark, which would run each version 100 times, by default):

```{r}
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 1L))
system.time(my_boot(dat = data.frame(x, y), my_stat, R = 4000, ncpus = 2L))
```


